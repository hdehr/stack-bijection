{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9618ee20-639c-4a22-a5f2-594dbafe241b",
   "metadata": {},
   "source": [
    "### Background\n",
    "Let us define a **stack** as a subset $S$ of a two-dimensional non-negative integer lattice such that\n",
    "1. if $(0, j)$ is in $S$ with $j > 0$ then $(0, j-1)$ is also in $S$ and\n",
    "2. if $(i, j)$ is in $S$ with $i,j > 0$ then either $(i-1, j)$ or $(i-1, j-1)$ is in $S$.\n",
    "\n",
    "Examples can be generated and displayed using the first 3 code cells.\n",
    "\n",
    "The central fact we are concerned with is that there are exactly $3^{n-1}$ stacks of size $n$.\n",
    "I became aware of this fact through Peter Kagey who made this related [StackExchange post](https://math.stackexchange.com/questions/3659431/number-of-ways-to-stack-lego-bricks), citing a 1988 paper by [Gouyou-Beauchamps and Viennot](https://doi.org/10.1016/0196-8858(88)90017-6).\n",
    "This project is an attempt to have a neural network learn an, ideally human-intelligible, bijection between stacks of size $n$ and ternary sequences of length $n-1$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f90cb25-86b1-4e2e-966d-33002ec9d39d",
   "metadata": {},
   "source": [
    "### Stack Generator\n",
    "\n",
    "First we need code to generate the stacks. The algorithm we use to do so operates by repeatedly choosing diagonal lines $d$ and placing a new cell in the greatest position $(i, i+d)$ along the diagonal that creates a valid stack. Consider that placing a cell in diagonal $d_1$ followed by $d_2$ may or may not result in the same stack as the reverse. That is, these additions to the stack may or may not \"commute\". To mitigate the probabilistic bias towards commuting additions, we choose each non-commuting addition for the next diagonal with probability $1/3$ (including d itself). If none of these diagonals are chosen, then a diagonal is chosen uniformly from one of the remaining valid ones. This scheme generates certain extremal stacks, such as the stack with a single row, with the \"correct\" probability of $3^{-n+1}$.\n",
    "\n",
    "One can optionally specify to the generator that stacks be formatted as \"unskewed\" so that cells $(i,j)$ are \"supported\" by either cells $(i-1, j)$ or $(i, j-1)$ rather than $(i-1, j)$ or $(i-1, j-1)$. This is more similar to the equivalent definition in the original 1988 paper. This format exhibits the inherent symmetry of the problem and also creates a more compact representation in that we can guarantee $i + j \\le n$ for any cell in the stack. I did not find this setting to make a noticible difference in learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ab54f-feab-4249-9b4e-aad62780714c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "\n",
    "class StackGenerator(torch.utils.data.IterableDataset):\n",
    "    \"\"\"\n",
    "    A pytorch dataset for generating stacks.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    limit -- the largest size of stack to generate\n",
    "    exact -- if true, generate only stacks of exactly the give size limit\n",
    "    seed -- optional random seed\n",
    "    skew -- if false, format output so that the stack \"rows\" actually lie along tensor antidiagonals\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, limit=40, exact=False, seed=None, skew=True):\n",
    "        super().__init__()\n",
    "        self.rand = random.Random(seed)\n",
    "        self.exact = exact\n",
    "        self.limit = limit\n",
    "        self.skew = skew\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            stack_set = {(0,0)}\n",
    "            height = {0: 1, -1: 0, -2: 0, 1: 0, 2: 0}\n",
    "            min_diag = -1\n",
    "            max_diag = 1\n",
    "            dependencies = [0, -1 ,1]\n",
    "            size = 1\n",
    "            while self.continue_condition(size):\n",
    "                size += 1\n",
    "                r = self.rand.randint(0, 2)\n",
    "                if r < len(dependencies):\n",
    "                    diag = dependencies[r]\n",
    "                else:\n",
    "                    diag = self.rand.randint(min_diag, max_diag)\n",
    "                    while diag in dependencies:\n",
    "                        diag = self.rand.randint(min_diag, max_diag)\n",
    "                row = max(height[diag], height[diag+1])\n",
    "                col = diag + row\n",
    "                stack_set.add((row,col))\n",
    "                height[diag] = row + 1\n",
    "                min_diag = min(diag - 1, min_diag)\n",
    "                max_diag = max(diag + 1, max_diag)\n",
    "                if min_diag - 1 not in height:\n",
    "                    height[min_diag - 1] = 0\n",
    "                if max_diag + 1 not in height:\n",
    "                    height[max_diag + 1] = 0\n",
    "                dependencies = [diag]\n",
    "                if height[diag - 1] < height[diag]:\n",
    "                    dependencies.append(diag-1)\n",
    "                if max(height[diag + 1], height[diag + 2]) + 1 > height[diag]:\n",
    "                    dependencies.append(diag+1)\n",
    "            \n",
    "            # Flip the stack left to right with probability 1/2\n",
    "            base_length = max([col for (row, col) in stack_set if row == 0])\n",
    "            if self.rand.random() < 1/2:\n",
    "                stack_set = {(row, base_length + row - col) for (row, col) in stack_set}\n",
    "\n",
    "            stack = torch.zeros([size, size], dtype=torch.bool)\n",
    "            if self.skew:\n",
    "                for row, col in stack_set:\n",
    "                    stack[row, col] = True\n",
    "            else:\n",
    "                for row, col in stack_set:\n",
    "                    stack[row - col + base_length, col] = True\n",
    "            yield stack\n",
    "\n",
    "    def continue_condition(self, count):\n",
    "        \"\"\"Determines whether to add additional cells.\"\"\"\n",
    "        if count >= self.limit:\n",
    "            return False\n",
    "        return self.exact or self.rand.random() < 1 - (count - 1) / count / 4\n",
    "        \n",
    "def collate_fn(data):\n",
    "    max_size = max(len(item) for item in data)\n",
    "    data = [F.pad(item, (0, max_size-len(item), 0, max_size-len(item)))[None] for item in data]\n",
    "    return torch.cat(data, 0)\n",
    "\n",
    "def worker_init(multiplier):\n",
    "    def fn(worker_id):\n",
    "        torch.utils.data.get_worker_info().dataset.rand.seed(multiplier*(worker_id+1))\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2fe9eb-cfe6-433a-81d6-f454b2fbe68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(StackGenerator(limit=40), batch_size=1, num_workers=0, collate_fn=collate_fn, worker_init_fn=worker_init(571))\n",
    "it = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7ad31-46cf-4762-af8f-7d869d1784df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(next(it)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da27034-3b2f-4035-a4d9-985c5dbd8e0f",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "In order to make a bijection representable as a neural network, we use the overall structure of a [variational autoencoder](https://en.wikipedia.org/wiki/Variational_autoencoder). That is, we have an **encoder** neural network that maps the input stack representation of size $n$ into a latent space of $n-1$ random samples drawn from [relaxed onehot categorical](https://pytorch.org/docs/stable/distributions.html#relaxedonehotcategorical) distributions, and a **decoder** neural network that maps the latent space back to a two-dimensional grid space. The **autoencoder** is the composition of these two modules. If the autoencoder achieves effectively perfect reconstruction while the latent distributions approach non-random values, then the encoder and decoder compute bijections between stacks and ternary sequences.\n",
    "\n",
    "In theory, achieving perfect reconstruction requires perfectly non-random distributions so the distributions need not be explicitly penalized in the loss function; we can simply use a measure of reconstruction error. Alternatively the autoencoder can be initialized with the `use_ELBO` flag set to `True` which will cause the autoencoder to return the [evidence lower bound](https://en.wikipedia.org/wiki/Evidence_lower_bound) during training which is intended to be used directly as the loss function.\n",
    "\n",
    "Internally, the encoder repeatedly applies the same residual block (i.e. with the same kernels) $n$ times, each time decreasing the grid size by 1. The $n$ vectors in the $(0,0)$ corner of the grid are then transformed into the distribution parameters. The decoder is very similar but in reverse. Unfortunately this makes the modules quite serial, but it ensures that the model scales to any input size $n$ without additional parameters, and does so homogeneously. Heuristically one might expect that this leads to a model that is more human-intelligible and *logically* scalable. However, this may be a premise worth revisiting in the hope that a denser, more parallel, but less homogeneous architecture can more easily find a solution that is still logically scalable. Alternatively the repeated portions of the modules could be made deeper, but this would slow training to even more of a crawl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b669a1-c605-4681-92c3-cdeaf05b0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.distributions.relaxed_categorical import RelaxedOneHotCategorical\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "import math\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "class BidirectionalMamba(nn.Module):\n",
    "    \"\"\"Splits input channel-wise and applies Mamba module in both directions.\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mamba = Mamba(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (x1, x2) = torch.split(x, x.size(1) // 2, dim=1)\n",
    "        x1 = torch.flip(x1, [2])\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.mamba(x.movedim(1,2)).movedim(2,1)\n",
    "        (x1, x2) = torch.split(x, x.size(1) // 2, dim=1)\n",
    "        x1 = torch.flip(x1, [2])\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        return x\n",
    "\n",
    "class DepthwiseResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, internal_dim, kernel_size=3, padding=1, glu=False, prenormalize=True):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(in_channels) if prenormalize else lambda x: x\n",
    "        self.proj_up = nn.Conv2d(in_channels, internal_dim, 1)\n",
    "        self.depth_conv = nn.Conv2d(internal_dim, internal_dim, kernel_size, groups=internal_dim, padding=padding)\n",
    "        self.gate = nn.GLU(dim=1) if glu else nn.SiLU()\n",
    "        self.proj_down = nn.Conv2d(internal_dim // (1 + glu), out_channels, 1)\n",
    "        self.id_proj = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else lambda x: x\n",
    "        size_change = 2 * padding - kernel_size + 1\n",
    "        if size_change == 0:\n",
    "            self.resize = lambda x: x\n",
    "        elif size_change < 0:\n",
    "            self.resize = lambda x: x[:, :, :size_change, :size_change]\n",
    "        else:\n",
    "            self.resize = nn.ZeroPad2d((0, size_change, 0, size_change))\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.norm(x.movedim(1, 3)).movedim(3, 1)\n",
    "        res = self.proj_up(res)\n",
    "        res = F.silu(res)\n",
    "        res = self.depth_conv(res)\n",
    "        res = self.gate(res)\n",
    "        res = self.proj_down(res)\n",
    "        return self.resize(self.id_proj(x)) + res\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, use_ELBO=False):\n",
    "        super().__init__()\n",
    "        self.state_size = 8\n",
    "        self.internal_dim = 64\n",
    "        \n",
    "        self.conv_block = DepthwiseResidualBlock(1, self.state_size, 2 * self.state_size, prenormalize=False)\n",
    "\n",
    "        self.repeat_block = DepthwiseResidualBlock(self.state_size, self.state_size, self.internal_dim, kernel_size=2, padding=0, glu=True)\n",
    "\n",
    "        self.flatnorm = nn.LayerNorm(self.state_size)\n",
    "        self.mamba = BidirectionalMamba(d_model=self.state_size, d_conv=3)\n",
    "        self.batchnorm = nn.BatchNorm1d(self.state_size)\n",
    "        self.shorten = nn.Conv1d(self.state_size, 3, 1)\n",
    "        self.temperature = nn.parameter.Parameter(torch.tensor([2/3]), requires_grad=False)\n",
    "        self.use_ELBO = use_ELBO\n",
    "    \n",
    "    def forward(self, x):\n",
    "        num_bricks = x.int().sum((1,2))\n",
    "        lower = torch.ones([x.size(1), x.size(1)], dtype=torch.bool, device=device).tril()\n",
    "        index = (num_bricks - 1).unsqueeze(1).expand(-1, x.size(1))\n",
    "        mask = torch.gather(lower, 0, index).unsqueeze(1)[:, :, 1:]\n",
    "\n",
    "        x = self.conv_block(x.unsqueeze(1))\n",
    "\n",
    "        logits = torch.zeros((x.size(0), self.state_size, x.size(2)-1), device=device)\n",
    "        for i in range(x.size(2)-1):\n",
    "            x = self.repeat_block(x)\n",
    "            logits[:, :, i] = x[:, :, 0, 0]\n",
    "        \n",
    "        logits = logits + self.mamba(self.flatnorm(logits.transpose(1, 2)).transpose(1, 2))\n",
    "        logits = self.batchnorm(logits)\n",
    "        logits = self.shorten(logits)\n",
    "\n",
    "        if self.training:\n",
    "            posterior = RelaxedOneHotCategorical(self.temperature, logits=logits.transpose(1, 2))\n",
    "            if self.use_ELBO:\n",
    "                return posterior, mask\n",
    "            out = posterior.rsample()\n",
    "        else:\n",
    "            out = torch.argmax(logits, dim=1)\n",
    "            out = F.one_hot(out, num_classes=3).float()\n",
    "        out = out.transpose(1, 2)\n",
    "        return out * mask\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.state_size = 8\n",
    "        self.internal_dim = 64\n",
    "        \n",
    "        self.lengthen = nn.Conv1d(3, self.state_size, 1)\n",
    "        self.batchnorm = nn.BatchNorm1d(self.state_size)\n",
    "        self.mamba = BidirectionalMamba(d_model=self.state_size, d_conv=3)\n",
    "\n",
    "        self.initial = nn.parameter.Parameter(torch.rand(1, self.state_size, 1, 1))\n",
    "\n",
    "        self.repeat_block = DepthwiseResidualBlock(2 * self.state_size, self.state_size, self.internal_dim, kernel_size=2, padding=1, glu=True)\n",
    "        \n",
    "        self.conv_block = DepthwiseResidualBlock(self.state_size, 1, 2 * self.state_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_bricks = torch.max(x != 0, dim=1).values.sum(1) + 1\n",
    "        lower = torch.ones([x.size(2) + 1, x.size(2) + 1], dtype=torch.bool, device=device).tril()\n",
    "        index = (num_bricks - 1).unsqueeze(1).expand(-1, x.size(2) + 1)\n",
    "        mask = torch.gather(lower, 0, index)\n",
    "        square_mask = torch.einsum('bh,bw->bhw', mask, mask).unsqueeze(1)\n",
    "        \n",
    "        x = self.lengthen(x)\n",
    "        x = x + self.mamba(self.batchnorm(x)) * mask.unsqueeze(1)[:,:,1:]\n",
    "\n",
    "        out = self.initial.expand(x.size(0), self.state_size, 1, 1)\n",
    "        for i in reversed(range(x.size(2))):\n",
    "            out = F.pad(out, (0, 0, 0, 0, self.state_size, 0))\n",
    "            out[:, :self.state_size, :, :] = x[:, :, i, None, None]\n",
    "            out = self.repeat_block(out)\n",
    "\n",
    "        out = self.conv_block(out)\n",
    "        \n",
    "        out = torch.where(square_mask, out, torch.finfo().min)\n",
    "        out = out.squeeze(1)\n",
    "        if self.training:\n",
    "            return out\n",
    "        return F.sigmoid(out)\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, use_ELBO=False):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(use_ELBO=use_ELBO)\n",
    "        self.dec = Decoder()\n",
    "        self.prior_temp = nn.parameter.Parameter(torch.tensor([1/10]), requires_grad=False)\n",
    "        self.prior_logits = nn.parameter.Parameter(torch.tensor([0.,0.,0.]), requires_grad=False)\n",
    "        self.use_ELBO = use_ELBO\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.use_ELBO:\n",
    "            posterior, mask = self.enc(x)\n",
    "            code = posterior.rsample()\n",
    "            likelihood_logits = self.dec(code.transpose(1, 2) * mask)\n",
    "            likelihood = Bernoulli(logits=likelihood_logits)\n",
    "            prior = RelaxedOneHotCategorical(self.prior_temp, logits=self.prior_logits)\n",
    "\n",
    "            log_prior = torch.sum(prior.log_prob(code) * mask[:,0,:], 1)\n",
    "            log_likelihood = torch.sum(likelihood.log_prob(x), (1, 2))\n",
    "            log_posterior = torch.sum(posterior.log_prob(code) * mask[:,0,:], 1)\n",
    "            unnormalized_ELBO = log_prior + log_likelihood - log_posterior\n",
    "            return -(unnormalized_ELBO / x.sum((1,2))).sum() / len(x)\n",
    "        \n",
    "        code = self.enc(x)\n",
    "        return code, self.dec(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b3947-5838-470f-a122-eba0f253b54d",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "There isn't too much to say about the training procedure. As stated above, we can use either evidence lower bound as a loss function, in which case the training loop below must be slightly modified, or a measure of how well the input is reconstructed. For training, this measure is the binary cross entropy, and for testing we count the number of grid cells that round to the incorrect value. In both cases we normalize by the input size $n$.\n",
    "\n",
    "If we are using reconstruction loss rather than the evidence lower bound, then we can use Newton's method as a root finding algorithm since we want to achieve **zero** reconstruction loss. Unfortunately, this has not yielded much success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4eb4f8-9f2e-4e9a-ab5e-845b96daebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import einops\n",
    "\n",
    "def reconstruction_loss(X, out):\n",
    "    unnormalized = F.binary_cross_entropy_with_logits(out, X, reduction='none').sum((1,2))\n",
    "    return (unnormalized / X.sum((1,2))).sum() / len(X)\n",
    "\n",
    "def reconstruction_error(X, reconstruction):\n",
    "    unnormalized = torch.round(torch.abs(X - reconstruction)).sum((1,2))\n",
    "    return (unnormalized / X.sum((1,2))).sum() / len(X)\n",
    "\n",
    "def train_loop(data_iter, model, num_batches=1000, reports=5):\n",
    "    model.train()\n",
    "    batches_per_report = int(num_batches / reports)\n",
    "    for i in range(num_batches):\n",
    "        X = next(data_iter).to(device, dtype=torch.float)\n",
    "        #loss = model(X)\n",
    "        _, out = model(X)\n",
    "        loss = reconstruction_loss(X, out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i % batches_per_report == 0:\n",
    "            print(f\"Loss: {loss.item():>10f}  [{(1 + i)*len(X)} samples]\")\n",
    "\n",
    "def test_loop(data_iter, model, num_batches=100):\n",
    "    model.eval()\n",
    "    recon_error = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_batches):\n",
    "            X = next(data_iter).to(device, dtype=torch.float)\n",
    "            code, reconstruction = model(X)\n",
    "            recon_error += reconstruction_error(X, reconstruction)\n",
    "    recon_error /= num_batches\n",
    "    print(f\"Average Error: {recon_error:>10f} \\n\")\n",
    "    return recon_error\n",
    "\n",
    "class NewtonOptimizer(torch.optim.Optimizer): \n",
    "\n",
    "    def __init__(self, params): \n",
    "        super().__init__(params, defaults={}) \n",
    "\n",
    "    def step(self, loss):\n",
    "        for group in self.param_groups:\n",
    "            params = [p for p in group['params'] if p.requires_grad]\n",
    "            grad, ps = einops.pack([p.grad.data for p in params], '*')\n",
    "            direction = - 1 * loss * grad / (torch.linalg.vector_norm(grad) ** 2 + 1e-4 )\n",
    "            direction = einops.unpack(direction, ps, '*')\n",
    "            for i, p in enumerate(params): \n",
    "                p.data += direction[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1debd029-abb5-4d2c-8dca-ce869af25901",
   "metadata": {},
   "source": [
    "The current settings have managed to achieve a test error of around 0.16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb40a46-df92-45a5-84e4-8169eff1336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "model = AutoEncoder(use_ELBO=False)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "#optimizer = NewtonOptimizer(model.parameters())\n",
    "dataloader = DataLoader(StackGenerator(limit=40), batch_size=64, num_workers=16, collate_fn=collate_fn, worker_init_fn=worker_init(51))\n",
    "data_iter = iter(dataloader)\n",
    "for i in range(40):\n",
    "    t = time.time()\n",
    "    train_loop(data_iter, model, num_batches=1000)\n",
    "    print(f\"Time Elapsed: {time.time()-t:>8f}\")\n",
    "    test_error = test_loop(data_iter, model, num_batches=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe23b9-da4f-484e-b216-c53e201af410",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "model.dec.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.dec(torch.tensor([[[1,0,0],[0,0,1],[0,0,1],[0,1,0],[0,0,1]]], device=device).float().movedim(2,1)).cpu()\n",
    "    plt.imshow(out.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1051c82-d55f-4938-ad3e-a09d234eb157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
